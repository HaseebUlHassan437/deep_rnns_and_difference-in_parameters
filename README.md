# Deep RNNs and Difference in Parameters

This repository contains a notebook exploring deep Recurrent Neural Network (RNN) architectures and their parameter differences, implemented using TensorFlow/Keras.

## Overview

The notebook investigates:

- How stacking multiple RNN layers (deep RNNs) compares to using wider single-layer RNNs.
- Differences in the number of trainable parameters across various RNN architectures.
- Practical insights into designing RNNs for sequential data using TensorFlow.
- Examples with both `SimpleRNN` and `LSTM` layers.

## Notebook Details

The notebook `deep_rnns_and_parameteric_difference.ipynb` includes:

- Building RNN and LSTM models with varying layers and hidden units using TensorFlow Keras.
- Calculating and comparing trainable parameters for different configurations.
- Analyzing the trade-offs between network depth and width.
- Visual demonstrations of parameter count differences.

## Usage

This notebook is ready to run on **Google Colab**:

1. Open [Google Colab](https://colab.research.google.com/).
2. Upload or open the notebook file `deep_rnns_and_parameteric_difference.ipynb`.
3. Run the notebook cells to explore deep RNN architectures and their parameter differences.

## Requirements

- TensorFlow is pre-installed in Google Colab, so no additional setup is needed.

## Author

Haseeb Ul Hassan

---

Feel free to explore and modify the notebook to deepen your understanding of deep RNN parameterization with TensorFlow!
